{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import itertools\n",
    "import logging\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import joblib\n",
    "import sys\n",
    "from main_model.model.pipeline import *\n",
    "from main_model.config.read_config import *\n",
    "from main_model.util.general_normalize import _clean_text\n",
    "from main_model.util.io_util import *\n",
    "from distances import get_most_similar_documents\n",
    "\n",
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
    "logging.root.level = logging.INFO\n",
    "\n",
    "\n",
    "PATH_DICTIONARY = \"models/id2word.dictionary\"\n",
    "PATH_CORPUS = \"models/corpus.mm\"\n",
    "PATH_LDA_MODEL = \"models/LDA.model\"\n",
    "PATH_DOC_TOPIC_DIST = \"models/doc_topic_dist.dat\"\n",
    "\n",
    "\n",
    "def head(stream, n=10):\n",
    "    \"\"\"\n",
    "    Return the first `n` elements of the stream, as plain list.\n",
    "    \"\"\"\n",
    "    return list(itertools.islice(stream, n))\n",
    "\n",
    "\n",
    "def make_texts_corpus(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield simple_preprocess(sentence, deacc=True)\n",
    "\n",
    "\n",
    "class StreamCorpus(object):\n",
    "    def __init__(self, sentences, dictionary, clip_docs=None):\n",
    "        \"\"\"\n",
    "        Parse the first `clip_docs` documents\n",
    "        Yield each document in turn, as a list of tokens.\n",
    "        \"\"\"\n",
    "        self.sentences = sentences\n",
    "        self.dictionary = dictionary\n",
    "        self.clip_docs = clip_docs\n",
    "\n",
    "    def __iter__(self):\n",
    "        for tokens in itertools.islice(make_texts_corpus(self.sentences),\n",
    "                                       self.clip_docs):\n",
    "            yield self.dictionary.doc2bow(tokens)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.clip_docs\n",
    "\n",
    "\n",
    "class LDAModel:\n",
    "\n",
    "    def __init__(self, num_topics, passes, chunksize,\n",
    "                 random_state=100, update_every=1, alpha='auto',\n",
    "                 per_word_topics=False):\n",
    "        \"\"\"\n",
    "        :param sentences: list or iterable (recommend)\n",
    "        \"\"\"\n",
    "\n",
    "        # data\n",
    "        self.sentences = None\n",
    "\n",
    "        # params\n",
    "        self.lda_model = None\n",
    "        self.dictionary = None\n",
    "        self.corpus = None\n",
    "\n",
    "        # hyperparams\n",
    "        self.num_topics = num_topics\n",
    "        self.passes = passes\n",
    "        self.chunksize = chunksize\n",
    "        self.random_state = random_state\n",
    "        self.update_every = update_every\n",
    "        self.alpha = alpha\n",
    "        self.per_word_topics = per_word_topics\n",
    "\n",
    "        # init model\n",
    "        # self._make_dictionary()\n",
    "        # self._make_corpus_bow()\n",
    "\n",
    "    def _make_corpus_bow(self, sentences):\n",
    "        self.corpus = StreamCorpus(sentences, self.id2word)\n",
    "        # save corpus\n",
    "        gensim.corpora.MmCorpus.serialize(PATH_CORPUS, self.corpus)\n",
    "\n",
    "    def _make_corpus_tfidf(self):\n",
    "        pass\n",
    "\n",
    "    def _make_dictionary(self, sentences):\n",
    "        self.texts_corpus = make_texts_corpus(sentences)\n",
    "        self.id2word = gensim.corpora.Dictionary(self.texts_corpus)\n",
    "        self.id2word.filter_extremes(no_below=10, no_above=0.25)\n",
    "        self.id2word.compactify()\n",
    "        self.id2word.save(PATH_DICTIONARY)\n",
    "\n",
    "    def documents_topic_distribution(self):\n",
    "        doc_topic_dist = np.array(\n",
    "            [[tup[1] for tup in lst] for lst in self.lda_model[self.corpus]]\n",
    "        )\n",
    "        # save documents-topics matrix\n",
    "        joblib.dump(doc_topic_dist, PATH_DOC_TOPIC_DIST)\n",
    "        return doc_topic_dist\n",
    "\n",
    "    def fit(self, sentences):\n",
    "        from itertools import tee\n",
    "        sentences_1, sentences_2 = tee(sentences)\n",
    "        self._make_dictionary(sentences_1)\n",
    "        self._make_corpus_bow(sentences_2)\n",
    "        self.lda_model = gensim.models.ldamodel.LdaModel(\n",
    "            self.corpus, id2word=self.id2word, num_topics=64, passes=5,\n",
    "            chunksize=100, random_state=42, alpha=1e-2, eta=0.5e-2,\n",
    "            minimum_probability=0.0, per_word_topics=False\n",
    "        )\n",
    "        self.lda_model.save(PATH_LDA_MODEL)\n",
    "\n",
    "    def transform(self, sentence):\n",
    "        \"\"\"\n",
    "        :param document: preprocessed document\n",
    "        \"\"\"\n",
    "        document_corpus = next(make_texts_corpus([sentence]))\n",
    "        corpus = self.id2word.doc2bow(document_corpus)\n",
    "        document_dist = np.array(\n",
    "            [tup[1] for tup in self.lda_model.get_document_topics(bow=corpus)]\n",
    "        )\n",
    "        return corpus, document_dist\n",
    "\n",
    "    def predict(self, document_dist):\n",
    "        doc_topic_dist = self.documents_topic_distribution()\n",
    "        return get_most_similar_documents(document_dist, doc_topic_dist)\n",
    "\n",
    "    def update(self, new_corpus):  # TODO\n",
    "        \"\"\"\n",
    "        Online Learning LDA\n",
    "        https://radimrehurek.com/gensim/models/ldamodel.html#usage-examples\n",
    "        https://radimrehurek.com/gensim/wiki.html#latent-dirichlet-allocation\n",
    "        \"\"\"\n",
    "        self.lda_model.update(new_corpus)\n",
    "        # get topic probability distribution for documents\n",
    "        for corpus in new_corpus:\n",
    "            yield self.lda_model[corpus]\n",
    "\n",
    "    def model_perplexity(self):\n",
    "        logging.INFO(self.lda_model.log_perplexity(self.corpus))\n",
    "\n",
    "    def coherence_score(self):\n",
    "        self.coherence_model_lda = gensim.models.coherencemodel.CoherenceModel(\n",
    "            model=self.lda_model, texts=self.corpus,\n",
    "            dictionary=self.id2word, coherence='c_v'\n",
    "        )\n",
    "        logging.INFO(self.coherence_model_lda.get_coherence())\n",
    "\n",
    "    def compute_coherence_values(self, mallet_path, dictionary, corpus,\n",
    "                                 texts, end=40, start=2, step=3):\n",
    "        \"\"\"\n",
    "        Compute c_v coherence for various number of topics\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        dictionary : Gensim dictionary\n",
    "        corpus : Gensim corpus\n",
    "        texts : List of input texts\n",
    "        end : Max num of topics\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        model_list : List of LDA topic models\n",
    "        coherence_values : Coherence values corresponding to the LDA model\n",
    "                           with respective number of topics\n",
    "        \"\"\"\n",
    "        coherence_values = []\n",
    "        model_list = []\n",
    "        for num_topics in range(start, end, step):\n",
    "            model = gensim.models.wrappers.LdaMallet(\n",
    "                mallet_path, corpus=self.corpus,\n",
    "                num_topics=self.num_topics, id2word=self.id2word)\n",
    "            model_list.append(model)\n",
    "            coherencemodel = gensim.models.coherencemodel.CoherenceModel(\n",
    "                model=model, texts=self.texts_corpus,\n",
    "                dictionary=self.dictionary, coherence='c_v'\n",
    "            )\n",
    "            coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "        return model_list, coherence_values\n",
    "\n",
    "    def plot(self, coherence_values, end=40, start=2, step=3):\n",
    "        x = range(start, end, step)\n",
    "        plt.plot(x, coherence_values)\n",
    "        plt.xlabel(\"Num Topics\")\n",
    "        plt.ylabel(\"Coherence score\")\n",
    "        plt.legend((\"coherence_values\"), loc='best')\n",
    "        plt.show()\n",
    "\n",
    "    def print_topics(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "def main():\n",
    "    # TODO\n",
    "    file_dir = os.path.dirname(__file__)\n",
    "    sys.path.append(file_dir)\n",
    "    # config = get_config()\n",
    "    json_content = read_config_file()\n",
    "\n",
    "    config = read_config_file()\n",
    "    df_extract = read_data(type_data='sql_server', config=config, path_data=None)\n",
    "    df_extract = pre_process_df(df_extract)\n",
    "    doc_tokenized = [simple_preprocess(doc) for doc in df_extract['clean_content']]\n",
    "\n",
    "    id2word = gensim.corpora.Dictionary(doc_tokenized)\n",
    " #   id2word.filter_extremes(no_below=20, no_above=0.1)\n",
    " #   id2word.compactify()\n",
    "\n",
    "    # save dictionary\n",
    "    id2word.save(PATH_DICTIONARY)\n",
    "\n",
    "    #ictionary = gensim.corpora.Dictionary()\n",
    "    corpus = [id2word.doc2bow(doc, allow_update=True) for doc in doc_tokenized]\n",
    "\n",
    "    #corpus = StreamCorpus(doc_tokenized, id2word)\n",
    "    # Term Document Frequency\n",
    "    #corpus = [id2word.doc2bow(text) for text in sentences]\n",
    "    # save corpus\n",
    "    gensim.corpora.MmCorpus.serialize(PATH_CORPUS, corpus)\n",
    "    # load corpus\n",
    "    # mm_corpus = gensim.corpora.MmCorpus('path_to_save_file.mm')\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(\n",
    "        corpus, num_topics=64, id2word=id2word, passes=10, chunksize=100\n",
    "    )\n",
    "    # save model\n",
    "    lda_model.save(PATH_LDA_MODEL)\n",
    "    #lda_model.print_topics(-1)\n",
    "    lda_model.show_topics(num_topics=10, num_words=20)\n",
    "    data = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "    pyLDAvis.display(data)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (4124226023.py, line 28)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  File \u001B[1;32m\"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_9984\\4124226023.py\"\u001B[1;36m, line \u001B[1;32m28\u001B[0m\n\u001B[1;33m    main()\u001B[0m\n\u001B[1;37m       ^\u001B[0m\n\u001B[1;31mIndentationError\u001B[0m\u001B[1;31m:\u001B[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "    test_doc = [\"dù dịch bệnh trên gia súc, gia cầm được kiểm soát tốt nhưng chăn nuôi lợn gặp khó khăn do giá bán thịt hơi vẫn ở mức thấp trong khi giá nguyên liệu chế biến thức ăn chăn nuôi tăng cao\"]\n",
    "    test_doc = [doc.split() for doc in test_doc]\n",
    "    test_corpus = [id2word.doc2bow(doc) for doc in test_doc]\n",
    "    from gensim.matutils import cossim\n",
    "    #doc1 = lda_model.get_document_topics(test_corpus[0], minimum_probability=0)\n",
    "    #doc2 = lda_model.get_document_topics(test_corpus[1], minimum_probability=0)\n",
    "    #print(cossim(doc1, doc2))\n",
    "\n",
    "    #new_doc_distribution = np.array([tup[1] for tup in lda_model.get_document_topics(test_corpus)])\n",
    "    new_doc_distribution = np.array([[tup[0] for tup in lst] for lst in lda_model[test_corpus]])\n",
    "    # we need to use nested list comprehension here\n",
    "    # this may take 1-2 minutes...\n",
    "    doc_topic_dist = np.array([[tup[1] for tup in lst] for lst in lda_model[corpus]])\n",
    "    doc_topic_dist.shape\n",
    "    # this is surprisingly fast\n",
    "\n",
    "    most_sim_ids = get_most_similar_documents(new_doc_distribution, doc_topic_dist)\n",
    "    most_similar_df = df_extract[df_extract.index.isin(most_sim_ids)]\n",
    "\n",
    "   # text = _clean_text(text)\n",
    "    #texts_corpus2 = make_texts_corpus(text)\n",
    "\n",
    "   # bow = gensim.corpora.Dictionary.doc2bow(texts_corpus2)\n",
    "   # for index, score in sorted(lda_model[bow], key=lambda tup: -1 * tup[1]):\n",
    "    #    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "np.random.seed(27)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}